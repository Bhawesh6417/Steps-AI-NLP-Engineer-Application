{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WfveIZ6KYmm"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "def is_valid_url(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    return parsed_url.scheme in (\"http\", \"https\")\n",
        "\n",
        "def crawl(url, depth, max_depth=5, visited=set()):\n",
        "    if depth > max_depth or url in visited:\n",
        "        return []\n",
        "\n",
        "    visited.add(url)\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "    data = [soup.get_text()]\n",
        "    links = soup.find_all('a', href=True)\n",
        "    for link in links:\n",
        "        next_url = urljoin(url, link['href'])\n",
        "        if is_valid_url(next_url):\n",
        "            data.extend(crawl(next_url, depth + 1, max_depth, visited))\n",
        "\n",
        "    return data\n",
        "\n",
        "# Start crawling from the main URL\n",
        "data = crawl(\"https://docs.nvidia.com/cuda/\", 0)\n",
        "\n",
        "# Print the first few data points to check the output\n",
        "for i, text in enumerate(data[:5]):\n",
        "    print(f\"Data {i+1}:\\n{text}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pymilvus\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Connect to MILVUS\n",
        "from pymilvus import connections, utility, Collection\n",
        "connections.connect(\"default\", host='localhost', port='19530')\n",
        "\n",
        "# Define the collection schema\n",
        "from pymilvus import FieldSchema, CollectionSchema, DataType\n",
        "\n",
        "fields = [\n",
        "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),\n",
        "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
        "    FieldSchema(name=\"metadata\", dtype=DataType.JSON)\n",
        "]\n",
        "schema = CollectionSchema(fields)\n",
        "collection = Collection(\"web_data\", schema)\n",
        "\n",
        "# Process data and insert into MILVUS\n",
        "for chunk in data:\n",
        "    embeddings = model.encode(chunk, convert_to_tensor=True)\n",
        "    collection.insert([range(len(embeddings)), embeddings, metadata])\n"
      ],
      "metadata": {
        "id": "IhTGNXrtLIKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Initialize Elasticsearch\n",
        "es = Elasticsearch()\n",
        "\n",
        "# Query expansion and retrieval\n",
        "def query_expansion(query):\n",
        "    # Implement query expansion logic\n",
        "    return [query]\n",
        "\n",
        "queries = query_expansion(\"Your query here\")\n",
        "results = es.search(index=\"web_data\", body={\"query\": {\"match\": {\"content\": queries}}})\n",
        "\n",
        "# Re-rank using a cross-encoder\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "scores = cross_encoder.predict([(query, result['_source']['content']) for result in results['hits']['hits']])\n",
        "re_ranked_results = sorted(zip(results['hits']['hits'], scores), key=lambda x: x[1], reverse=True)\n"
      ],
      "metadata": {
        "id": "vpzg5j9tLLYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "def answer_question(question, context):\n",
        "    return qa_pipeline(question=question, context=context)\n",
        "\n",
        "# Use the highest-ranked context for answering\n",
        "answer = answer_question(\"Your question here\", re_ranked_results[0][0]['_source']['content'])\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "OsrWNrdyLN8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "st.title(\"Question Answering System\")\n",
        "user_query = st.text_input(\"Enter your query:\")\n",
        "if st.button(\"Get Answer\"):\n",
        "    answer = answer_question(user_query, re_ranked_results[0][0]['_source']['content'])\n",
        "    st.write(answer)\n"
      ],
      "metadata": {
        "id": "oSynZwG8LTgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}